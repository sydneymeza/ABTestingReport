<!-- TO RUN IN BROWSER:
    1. Install vscode extension Name: open in `browser open-in-browser` by TechER
    2. Right click html file and either run in default browser or another browser
  -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AB Testing Report</title>
    <!-- import CSS styles -->
    <link rel="stylesheet" href="styles.css" />
  </head>

  <!-- page content goes into <body> -->
  <body>
    <div id="logoBar">
      <h1>AB Testing Assignment!</h1>
      <h2>:D</h2>
    </div>
    <hr />
    <div id="Overview">
      <h1>Overview</h1>
      <p>
        Actually finding how effecitive a small change can make is very
        difficult, luckily we cant use AB testing/statistics to do this. For
        this assignment I was able to test two differnt interfaces, one that was
        prepared for me to use, and the other is the one with the changes that I
        made. This was a great opportunity to learn about AB testing and see
        what information can be found from using statistics and different
        methods.
      </p>
      <p>
        The parts of the website that I chose to redesign include changing the
        colors, of the buttons, the arrangment of the buttons, the arrangement
        of the appointment details, and the arrangement of the appointments
        themselves I wanted to change the button colors t omake it more clear
        which button was the'make an appointment' button, and which one was to
        view the details. I also changed the arrangement so that the buttons
        were in a row rather than set in a column. The reasoning behind this was
        to allow for more clickable space so that there are no accidental
        clicks. I rearranged the appointment details by having the doctor's name
        be the first detail that is seen, it's now the appointment header rather
        than what type of appointment it is. I rearranged the appointments by
        having them be in ascending order by dates rather than a bunch of dates
        mixed up. I had this idea so that it would be easier to see the dates in
        order.
      </p>
    </div>
    <div id="part1">
      <h1>Data Collection</h1>
      <p>
        I took part and my classmates took part in this AB testing assignment.
        We first all 'tested' the A verison of the interface by being assigned a
        task, which was to schedule an appointment with a specific doctor, on a
        specific day, in a specific place. All my classmates were assigned the
        same tasks and that is where the data comes for the A version. After I
        was able to make changes to the website, and then we tested each others
        websites, so the same people tested both version A and B of this test.
      </p>
      <div class="photos">
        <img
          alt="image of version B of the website, or the website that I made changes to"
          src="./assets/versionB.jpg"
        />
      </div>
      <p>Here is the data that was collected:</p>
      <div class="photos">
        <img
          alt="image of the csv file that contains data for the version A testing, it contains columns with uid, version, timestamp_load, time_on_page, time_to_first_click, mouse_move_distance, num_clicks, did_misclick, did_succeed"
          src="./assets/versionA_data.jpg"
        />
        <img
          alt="image of the csv file that contains data for the version B testing, it contains columns with uid, version, timestamp_load, time_on_page, time_to_first_click, mouse_move_distance, num_clicks, did_misclick, did_succeed"
          src="./assets/versionB_data.jpg"
        />
      </div>
      <p>What each of these columns represent is:</p>
      <li>uid: Unique identifier for each user.</li>
      <li>
        version: Version (A or B) of the website that the user interacted with.
      </li>
      <li>
        timestamp_load: The time (in milliseconds) elapsed since the epoch.
      </li>
      <li>
        time_on_page: Total time (in milliseconds) that the user spent on the
        page.
      </li>
      <li>
        time_to_first_click: The time (in milliseconds) it took for the user to
        execute his/her first click.
      </li>
      <li>
        mouse_move_distance: The total distance (in pixels) of the userâ€™s mouse
        movements.
      </li>
      <li>num_clicks: The number of times the user clicked on the screen.</li>
      <li>
        did_misclick: A boolean flag indicating if the user pushed a button
        external to the task.
      </li>
      <li>
        did_succeed: A boolean flag indicating if the user successfully
        completed the task.
      </li>
    </div>
    <div id="part2">
      <h1>Hypothesis</h1>
      <p>
        The data points that I chose to create hypothesis for include, the
        misclick rate, the time spent on page, and the total distance (in
        pixels) that the mouse moved.
      </p>

      <table>
        <tr>
          <th></th>
          <th>Misclick</th>
          <th>Time Spent on Page</th>
          <th>Mouse Movement Distance</th>
        </tr>
        <tr>
          <th>Null Hypothesis</th>
          <td>The rate of misclick is the same for both versions</td>
          <td>
            The average time spent on the page are the same for both versions
          </td>
          <td>
            The average mouse movement distance of each user is the same for
            both versions
          </td>
        </tr>
        <tr>
          <th>Alternative Hypothesis</th>
          <td>Version B has a lower rate of misclicks than version A</td>
          <td>
            Version B has a lower time spent on page average than version A
          </td>
          <td>
            Version B has a lower average mouse movement distance average than
            version A
          </td>
        </tr>
      </table>
      <div id="part3">
        <h1>Analysis</h1>
        <h2>Misclick Rate</h2>
        <p>
          The reasoning behind the alternative hypothesis is because I made the
          clickable area larger and I made the contrast between the two buttons
          more noticeable, I hope that this will create a noticeable difference
          between the two versions.
        </p>
        <p>
          For the misclick rate I do predict that I will be able to reject the
          null hypothesis.
        </p>
        <p>
          The type of test that I chose to use for this metric is the two-tailed
          T test. I thought that this would be the best test for this metric
          because I wanted to compare the two averages. I know that since I'm
          simply comparing whether or not one average is different from the
          other average, I could have thought of another way to compare, or to test the misclicks, but this is what made most sense to me. 

          <br />
          The average percent of people that made a misclick for version A is
          26%. The percent of people that made a misclick on version B is 4%. To find whether or not I can reject the null hypothesis, I chose to use the alpha value of 0.05. Thus to be able to reject the null hypothesis the p value from this test has to be less than the alpha value. 
          After using the script given to me for the two-tailed t test, the p-value of the test is 0.04.
          The p-value is 0.04 which is less than then alpha thus we can reject
          the null hypothesis. However this test did not show that there is significant difference between the two versions. 
        </p>
        <h2>Time spent on Page</h2>
        <p>
          The reasoning behind the alternative hypothesis is because I hopefully
          made the page easier to scan. I highlighted the dates, and put them in
          an easier order to read, and I also changed the format of the
          appointment details. Thus hopefully the time spent on the page is less
          than version A because the user will have an easier time scanning the
          page for the information that they need.
        </p>
        <p>
          For the time spent on page I do predict that I will be able to reject
          the null hypothesis.
        </p>
        <p>
          The type of test that I chose to use for this metric is the two-tailed
          test. I thought that this would be the best test for this metric
          because I wanted to compare the two averages. I know that since I'm
          simply comparing whether or not one average is different from the
          other average, I could have thought of another way to compare, or to test the misclicks, but this is what made most sense to me. 

          <br />
          The average time spent on the page of version A is 24398.28 seconds. The average
          time spent on the page of version B is 7799.29 seconds. To find whether or not I can reject the null hypothesis, I chose to use the alpha value of 0.05. Thus to be able to reject the null hypothesis the p value from this test has to be less than the alpha value. 
          After using the script given to me for the two-tailed t test, the p-value of the test is 0.000035. The p-value is 0.000035 which is 
          significantly less than then alpha thus we can reject the null hypothesis. There is a statistically significant different between the two. 
        </p>
        <h2>Distance of mouse movement (in pixels)</h2>
        <p>
          The reasoning behind the alternative hypothesis is similar to the time
          spent on page reasoning. Because users will hopefully spend less time
          scanning, this means that they will also hopefully have a lower mouse
          movement when they are scanning the page and searching for where to
          click.
        </p>
        <p>
          For the mouse movement distance I do predict that I will fail to reject the null
          hypothesis.
        </p>
        <p>
          The type of test that I chose to use for this metric is the two-tailed
          test. I thought that this would be the best test for this metric
          because I wanted to compare the two averages. I know that since I'm
          simply comparing whether or not one average is different from the
          other average, I could have thought of another way to compare, or to test the distance, but this is what made most sense to me. 

          <br />
          The average mouse movement distance of version A is 6458.7 pixels. The
          average mouse movement distance of version B is 2340.3 pixels. To find whether or not I can reject the null hypothesis, I chose to use the alpha value of 0.05. Thus to be able to reject the null hypothesis the p value from this test has to be less than the alpha value. 
          After using the script given to me for the two-tailed t test, the p-value of the test is 0.00014. The p-value is 0.00014 which is 
          significantly less than then alpha thus we can reject the null hypothesis. There is a statistically significant different between the two versions. 
          <br>
          I didn't expect my prediction to be false, but I guess that the faster scanning patter allowed for smaller mouse movement distance. 
        </p>
      </div>
    </div>
    <div id="part3">
        <h1>Summary Statistics</h1>
        <h2>Misclick Rate</h2>
        <p>
          Since I collected about 20 data points, the average amount of people that misclicked in version A was about 26%. The variance is 20.1%. 
          The average version B had was 4% with a variance of 4%. Thus version B has a significantly smaller misclick rate than version A.
        </p>
        <h2>Time spent on Page</h2>
        <p>
          Since I collected about 20 data points, the average user for version A had an average time of 24398.28 seconds. The variance is 206407623 seconds. 
          The average user for version B had a mouse movement distance of about 7799.29 seconds. The variance is 4433968.91. Thus version B had an average time less than half the average of version A. 
        </p>
        <h2>Distance of mouse movement (in pixels)</h2>
        <p>
          Since I collected about 20 data points, the average user for version A had a mouse movement distance of about 6458.7 pixels. The variance is 15409604.8 pixels. 
          The average user for version B had a mouse movement distance of about 2430.3 pixels. The variance is 410148.98 pixels. Thus version B had an average distance half the average of version A. 
        </p>
      </div>
    </div>

    <div id="part4">
      <h1>Conclusion</h1>
      <p>
        I feel comfortable using the tests that I did do to analyze A/B testing.
        However since I didn't use a large variety of tests, I hope to learn
        more about the other tests in the future so that I can use the data more
        extensively. As the tests that I did for this assignment were
        informative, but I believe that there is other information that I could
        have seen had I analyzed or tested the data in a different way.
      </p>
    </div>
  </body>
</html>
